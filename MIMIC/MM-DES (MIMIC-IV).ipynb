{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe286a12-1d04-4b2e-a339-2aa46fb20576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7bd46c-8359-4e6a-9c66-95fc8b8211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTabularDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        text_col=\"text\",\n",
    "        label_col=\"sepsis\",\n",
    "        drop_cols=(\"text\", \"hadm_id\", \"sepsis\"),\n",
    "        tokenizer=None,\n",
    "        max_len=256\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas DataFrame (train / val / test)\n",
    "            text_col: column containing clinical text\n",
    "            label_col: target label (sepsis)\n",
    "            drop_cols: columns to exclude from tabular features\n",
    "            tokenizer: HuggingFace tokenizer\n",
    "            max_len: max token length for text\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.drop_cols = drop_cols\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Identify tabular feature columns automatically\n",
    "        self.tabular_cols = [\n",
    "            c for c in df.columns if c not in drop_cols\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- TEXT --------\n",
    "        text = str(row[self.text_col])\n",
    "        text_enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # -------- TABULAR --------\n",
    "        tab_features = torch.tensor(\n",
    "            [float(row[c]) for c in self.tabular_cols],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # -------- LABEL --------\n",
    "        label = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"tab\": tab_features,\n",
    "            \"label\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4065239-53a5-494c-89a0-343344077953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/firuz/Firuz/MM-DES/MIMIC\n",
      "True\n",
      "['biobert_sepsis', 'clinicalbert_sepsis', 'pubmedbert_sepsis']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.path.exists(\"saved_text_models/clinicalbert_sepsis\"))\n",
    "print(os.listdir(\"saved_text_models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3876e7-6116-4d48-8a9f-ad16ed90dfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'model.safetensors']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"saved_text_models/clinicalbert_sepsis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391b3cfe-d2b9-4d9c-8cd6-5d27c8aa11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "assert os.path.exists(\"saved_text_models/clinicalbert_sepsis\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"saved_text_models/clinicalbert_sepsis\",\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819ef234-be6e-4b20-8a87-b740d407a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df   = pd.read_csv(\"data/dsel.csv\")\n",
    "test_df  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_ds = TextTabularDataset(train_df, tokenizer=tokenizer)\n",
    "val_ds   = TextTabularDataset(val_df, tokenizer=tokenizer)\n",
    "test_ds  = TextTabularDataset(test_df, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fa6dd-9fc5-49c2-aa3d-47a6c58d5b96",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ab0afa-0494-42de-ab01-760db15fc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "610995c9-7812-44e2-9b4b-7ef5b7e36567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTabularEmbeddingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_path,\n",
    "        tab_input_dim,\n",
    "        tab_hidden_dim=128,\n",
    "        embed_dim=128,\n",
    "        freeze_text=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- TEXT ENCODER ----------\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_path)\n",
    "\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        text_hidden_dim = self.text_encoder.config.hidden_size\n",
    "        self.text_proj = nn.Linear(text_hidden_dim, embed_dim)\n",
    "\n",
    "        # ---------- TABULAR ENCODER ----------\n",
    "        self.tabular_encoder = nn.Sequential(\n",
    "            nn.Linear(tab_input_dim, tab_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(tab_hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # ---------- LOGIT SCALE ----------\n",
    "        self.logit_scale = nn.Parameter(\n",
    "            torch.ones([]) * np.log(1 / 0.07)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tabular):\n",
    "        # Text embedding\n",
    "        text_out = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        cls_embed = text_out.last_hidden_state[:, 0]\n",
    "        text_embed = self.text_proj(cls_embed)\n",
    "\n",
    "        # Tabular embedding\n",
    "        tab_embed = self.tabular_encoder(tabular)\n",
    "\n",
    "        # Normalize (Symile expects normalized embeddings)\n",
    "        text_embed = F.normalize(text_embed, dim=1)\n",
    "        tab_embed = F.normalize(tab_embed, dim=1)\n",
    "\n",
    "        return [text_embed, tab_embed], self.logit_scale.exp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3ef1818-e0fc-4e0d-ba79-59cb2fa70649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_embedding_model(\n",
    "    embedding_model,\n",
    "    train_loader,\n",
    "    val_loader=None,\n",
    "    loss_fn=None,\n",
    "    optimizer=None,\n",
    "    device=\"cuda\",\n",
    "    num_epochs=20,\n",
    "    print_every=1\n",
    "):\n",
    "    embedding_model.to(device)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        embedding_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            tab = batch[\"tab\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embeds, logit_scale = embedding_model(\n",
    "                input_ids, attention_mask, tab\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(embeds, logit_scale)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        history.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            if val_loader is not None:\n",
    "                embedding_model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        input_ids = batch[\"input_ids\"].to(device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                        tab = batch[\"tab\"].to(device)\n",
    "\n",
    "                        embeds, logit_scale = embedding_model(\n",
    "                            input_ids, attention_mask, tab\n",
    "                        )\n",
    "                        val_loss += loss_fn(embeds, logit_scale).item()\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "                embedding_model.train()\n",
    "\n",
    "    return embedding_model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29875d56-2368-40ee-b814-9f4669a5803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████████████████| 357/357 [00:31<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 2.7993\n",
      "Validation Loss: 2.7627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|████████████████████████████████████████████| 357/357 [00:31<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 2.7732\n",
      "Validation Loss: 2.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|████████████████████████████████████████████| 357/357 [00:31<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 2.7722\n",
      "Validation Loss: 2.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|████████████████████████████████████████████| 357/357 [00:30<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 2.7712\n",
      "Validation Loss: 2.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|████████████████████████████████████████████| 357/357 [00:31<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 2.7712\n",
      "Validation Loss: 2.7625\n"
     ]
    }
   ],
   "source": [
    "from symile import Symile\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model = TextTabularEmbeddingModel(\n",
    "    text_model_path=\"saved_text_models/clinicalbert_sepsis\",\n",
    "    tab_input_dim=train_ds[0][\"tab\"].shape[0],\n",
    "    freeze_text=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, embedding_model.parameters()),\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "symile_loss_fn = Symile()\n",
    "\n",
    "embedding_model, history = train_embedding_model(\n",
    "    embedding_model=embedding_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,   # DSEL is perfect here\n",
    "    loss_fn=symile_loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8ded0-3fb8-40f4-a2a7-1ec1931a7e94",
   "metadata": {},
   "source": [
    "### DSEL (RoC Defining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee972135-6211-49a4-8051-ec76ff27f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit_dsel_embeddings(embedding_model, dsel_loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute text and tabular embeddings for the DSEL dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dsel_embeddings: dict with 'text' and 'tab'\n",
    "        logit_scale_exp: learned temperature\n",
    "    \"\"\"\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "\n",
    "    dsel_embeddings = {\n",
    "        \"text\": [],\n",
    "        \"tab\": []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dsel_loader, desc=\"Computing DSEL embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            tab = batch[\"tab\"].to(device)\n",
    "\n",
    "            embeds, logit_scale_exp = embedding_model(\n",
    "                input_ids, attention_mask, tab\n",
    "            )\n",
    "\n",
    "            text_embed, tab_embed = embeds\n",
    "\n",
    "            dsel_embeddings[\"text\"].append(\n",
    "                F.normalize(text_embed, dim=1)\n",
    "            )\n",
    "            dsel_embeddings[\"tab\"].append(\n",
    "                F.normalize(tab_embed, dim=1)\n",
    "            )\n",
    "\n",
    "    # Concatenate\n",
    "    dsel_embeddings[\"text\"] = torch.cat(dsel_embeddings[\"text\"], dim=0)\n",
    "    dsel_embeddings[\"tab\"]  = torch.cat(dsel_embeddings[\"tab\"], dim=0)\n",
    "\n",
    "    return dsel_embeddings, logit_scale_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d1d39-8bd4-4cfd-a1ae-16f8e11b56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_faiss(\n",
    "    embedding_model,\n",
    "    test_sample,\n",
    "    faiss_index_text,\n",
    "    faiss_index_tab,\n",
    "    logit_scale_exp,\n",
    "    k=7,\n",
    "    weights={\"text\": 0.5, \"tab\": 0.5},\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Region of Competence (RoC) using FAISS for fast NN search.\n",
    "    Assumes FAISS indices are built on normalized DSEL embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # --- Prepare test input ---\n",
    "    input_ids = test_sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = test_sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "    tab = test_sample[\"tab\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeds, _ = embedding_model(input_ids, attention_mask, tab)\n",
    "        text_embed, tab_embed = embeds\n",
    "\n",
    "        # Normalize → cosine similarity via inner product\n",
    "        text_embed = F.normalize(text_embed, dim=1)\n",
    "        tab_embed  = F.normalize(tab_embed,  dim=1)\n",
    "\n",
    "    # --- FAISS search ---\n",
    "    q_text = text_embed.cpu().numpy()\n",
    "    q_tab  = tab_embed.cpu().numpy()\n",
    "\n",
    "    D_text, I_text = faiss_index_text.search(q_text, k)\n",
    "    D_tab,  I_tab  = faiss_index_tab.search(q_tab,  k)\n",
    "\n",
    "    # --- Fuse similarities ---\n",
    "    scores = (\n",
    "        weights[\"text\"] * D_text +\n",
    "        weights[\"tab\"]  * D_tab\n",
    "    ) * logit_scale_exp\n",
    "\n",
    "    return I_text[0], scores[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce746538-821c-488d-a671-5946b1a68318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(\n",
    "    embedding_model,\n",
    "    test_sample,\n",
    "    dsel_embeddings,\n",
    "    logit_scale_exp,\n",
    "    k=7,\n",
    "    weights={\"text\": 0.5, \"tab\": 0.5},\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Region of Competence (RoC) for a test sample.\n",
    "    \"\"\"\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # --- Prepare test input ---\n",
    "    input_ids = test_sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = test_sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "    tab = test_sample[\"tab\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeds, _ = embedding_model(\n",
    "            input_ids, attention_mask, tab\n",
    "        )\n",
    "        text_embed, tab_embed = embeds\n",
    "\n",
    "        text_embed = F.normalize(text_embed, dim=1)\n",
    "        tab_embed  = F.normalize(tab_embed, dim=1)\n",
    "\n",
    "    # --- Similarity computation ---\n",
    "    sim_text = logit_scale_exp * torch.matmul(\n",
    "        text_embed, dsel_embeddings[\"text\"].T\n",
    "    )\n",
    "    sim_tab = logit_scale_exp * torch.matmul(\n",
    "        tab_embed, dsel_embeddings[\"tab\"].T\n",
    "    )\n",
    "\n",
    "    similarity_scores = (\n",
    "        weights[\"text\"] * sim_text +\n",
    "        weights[\"tab\"] * sim_tab\n",
    "    ).squeeze()\n",
    "\n",
    "    # --- Top-K neighbors ---\n",
    "    k = min(k, similarity_scores.numel())\n",
    "    topk_values, topk_indices = torch.topk(similarity_scores, k)\n",
    "\n",
    "    return topk_indices.cpu().numpy(), topk_values.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff7640f8-d03a-4494-a650-eaaa6e505465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing DSEL embeddings: 100%|██████████████████████████████| 77/77 [00:04<00:00, 18.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Top-K DSEL indices: [1101 1207  227  310  729 1151  817]\n",
      "Similarity scores: [13.640659  13.6345005 13.617228  13.615385  13.610577  13.609999\n",
      " 13.607555 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute DSEL embeddings\n",
    "dsel_embeddings, logit_scale_exp = fit_dsel_embeddings(\n",
    "    embedding_model,\n",
    "    val_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Step 2: Pick a test sample\n",
    "test_sample = test_ds[100]\n",
    "\n",
    "# Step 3: Get RoC\n",
    "topk_indices, topk_scores = get_roc(\n",
    "    embedding_model,\n",
    "    test_sample,\n",
    "    dsel_embeddings,\n",
    "    logit_scale_exp,\n",
    "    k=7,\n",
    "    weights={\"text\": 0.6, \"tab\": 0.4}\n",
    ")\n",
    "\n",
    "print(f\"True label: {test_sample['label']}\")\n",
    "print(\"Top-K DSEL indices:\", topk_indices)\n",
    "print(\"Similarity scores:\", topk_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86ffa6a3-dfeb-4bf5-a185-57cd3c68380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topk_text_from_tokens(\n",
    "    test_sample,\n",
    "    topk_indices,\n",
    "    topk_scores,\n",
    "    dsel_dataset,\n",
    "    tokenizer,\n",
    "    max_chars=400\n",
    "):\n",
    "    \"\"\"\n",
    "    Print test sample and Top-K DSEL samples by decoding input_ids.\n",
    "    \"\"\"\n",
    "\n",
    "    def decode(sample):\n",
    "        return tokenizer.decode(\n",
    "            sample[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "    def truncate(txt, n=max_chars):\n",
    "        return txt[:n] + (\"...\" if len(txt) > n else \"\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"TEST SAMPLE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"Label: {test_sample['label'].item() if torch.is_tensor(test_sample['label']) else test_sample['label']}\")\n",
    "    print(\"-\" * 120)\n",
    "    print(truncate(decode(test_sample)))\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    print(\"\\nTOP-K REGION OF COMPETENCE (DSEL)\")\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    for rank, (idx, score) in enumerate(zip(topk_indices, topk_scores), start=1):\n",
    "        dsel_sample = dsel_dataset[idx]\n",
    "\n",
    "        print(f\"\\n--- Rank {rank} ---\")\n",
    "        print(f\"DSEL index: {idx}\")\n",
    "        print(f\"Label: {dsel_sample['label'].item() if torch.is_tensor(dsel_sample['label']) else dsel_sample['label']}\")\n",
    "        print(f\"Similarity score: {float(score):.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(truncate(decode(dsel_sample)))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    print(\"=\" * 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7596d4ce-539b-4ecb-8f44-2ed295b2196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "TEST SAMPLE\n",
      "========================================================================================================================\n",
      "Label: 1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "indication : acute renal insufficiency. history of esophageal mass. technique : renal sonogram. comparison : none available. findings : the right kidney measures 10. 7 cm and appears within normal limits without stones, masses or overt hydronephrosis. the left kidney measures 11. 6 cm and demonstrates moderate hydronephrosis and proximal hydroureter level to which it can be traced on this examinat...\n",
      "========================================================================================================================\n",
      "\n",
      "TOP-K REGION OF COMPETENCE (DSEL)\n",
      "========================================================================================================================\n",
      "\n",
      "--- Rank 1 ---\n",
      "DSEL index: 1101\n",
      "Label: 0\n",
      "Similarity score: 13.6407\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : desaturation to 60 %. comparison : no relevant comparisons available. findings : limited study due to patient position and 2 ap portable semi - upright images were taken. there are diffuse bilateral pulmonary opacities, worst at the bases with indistinctness of the pulmonary vasculature, suggestive of pulmonary edema. there is probably a left pleural effusion. the heart size cannot be...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 2 ---\n",
      "DSEL index: 1207\n",
      "Label: 0\n",
      "Similarity score: 13.6345\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : dyspnea, tachypnea, evaluate underlying cardiopulmonary process. comparison : _ _ _. ap semi - upright portable radiograph of the chest : the technique is limited due to motion blur. the left costophrenic angle is excluded from the field of view. the lungs are clear with the exception of mild opacity in the left lower lobe, suggestive of atelectasis. there is no focal consolidation, p...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 3 ---\n",
      "DSEL index: 227\n",
      "Label: 1\n",
      "Similarity score: 13.6172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : shortness of breath and hypoxia. evaluate for pneumonia. comparison : chest radiographs _ _ _ through _ _ _. cta chest _ _ _. technique : portable upright ap radiograph of the chest. findings : the patient is notably right - ward rotated, limiting the evaluation. heart size is not well evaluated but enlarged. lung volumes are low. bibasilar opacities may reflect atelectasis, although ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 4 ---\n",
      "DSEL index: 310\n",
      "Label: 0\n",
      "Similarity score: 13.6154\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : _ _ _ year old man with dyspnea in setting of mi / /? pulmonary edema technique : ap portable chest radiograph comparison : _ _ _ from earlier in the day impression : the lungs are hyperexpanded. there is persisting mild pulmonary edema. bilateral parenchymal opacities are again noted particularly in the left mid and lower lung zones as well as in the right upper lobe and right lung b...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 5 ---\n",
      "DSEL index: 729\n",
      "Label: 1\n",
      "Similarity score: 13.6106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : _ _ _ woman with dyspnea on exertion and hypoxia ; new cancer diagnosis. comparison : none. ap and lateral semi - upright views of the chest at 3 : 30 p. m : there is a large left pleural effusion, with fluid filling the left hemithorax and obscuring most of the left lung. this is associated with extensive atelectasis. a small region of left lung in the upper lobe remains aerated. the...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 6 ---\n",
      "DSEL index: 1151\n",
      "Label: 0\n",
      "Similarity score: 13.6100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : _ _ _ year old woman with dyspnea after vomit / /? pna technique : single portable view of the chest comparison : chest x - ray from _ _ _. findings : since prior, there has been interval development of right basilar opacity which is in part due to underlying effusion and atelectasis. infection would certainly be possible. left lung is clear. cardiac silhouette is within normal limits...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Rank 7 ---\n",
      "DSEL index: 817\n",
      "Label: 1\n",
      "Similarity score: 13.6076\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indication : shortness of breath and hypoxia. evaluate for pulmonary embolism. comparisons : cta of the chest from _ _ _. ct of the chest from _ _ _. technique : contiguous axial mdct images were obtained through the chest after the administration of iv contrast per the chest pain protocol. sagittal, coronal, and oblique reformatted images were obtained and reviewed. total dlp : 231. 69 mgy - cm. ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_topk_text_from_tokens(\n",
    "    test_sample=test_sample,\n",
    "    topk_indices=topk_indices,\n",
    "    topk_scores=topk_scores,\n",
    "    dsel_dataset=val_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150cdb8-c4e8-46b5-bdf9-c5b245d4231b",
   "metadata": {},
   "source": [
    "### Load Base Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "910b81bf-98bb-40de-bd87-757487ae8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709, 74) (5709,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_tab = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    sample = train_ds[i]\n",
    "    X_train_tab.append(sample[\"tab\"].numpy())\n",
    "    y_train.append(sample[\"label\"].item())\n",
    "\n",
    "X_train_tab = np.array(X_train_tab)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train_tab.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1d8a96-7444-4600-993b-d1a50ded937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_tab_scaled = scaler.fit_transform(X_train_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b63e39f-f98c-42c4-aab6-e635883bfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200, random_state=42\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=200, verbose=0, random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"MLP\": MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ece2c4a-7936-4aa9-bfba-04a3d1f372f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models:   0%|                                          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models:  20%|██████▊                           | 1/5 [00:03<00:14,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CatBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models:  40%|█████████████▌                    | 2/5 [00:04<00:05,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models:  60%|████████████████████▍             | 3/5 [00:04<00:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models:  80%|███████████████████████████▏      | 4/5 [00:07<00:01,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tabular models: 100%|██████████████████████████████████| 5/5 [00:10<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "base_tabular_models = []\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Training tabular models\"):\n",
    "    print(f\"\\nTraining {name}\")\n",
    "    model.fit(X_train_tab_scaled, y_train)\n",
    "    base_tabular_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1313ca-7baa-4cb0-be1c-7e072c977848",
   "metadata": {},
   "source": [
    "### Language Base models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9b3c501-b2c4-411f-b5cf-97497f4b5578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading biobert_sepsis...\n",
      "Loading clinicalbert_sepsis...\n",
      "Loading pubmedbert_sepsis...\n",
      "\n",
      "Loaded 3 language models\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_language_models = []\n",
    "base_language_tokenizers = []\n",
    "\n",
    "MODEL_ROOT = \"saved_text_models\"\n",
    "\n",
    "for model_dir in sorted(os.listdir(MODEL_ROOT)):\n",
    "    model_path = os.path.join(MODEL_ROOT, model_dir)\n",
    "\n",
    "    print(f\"Loading {model_dir}...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    base_language_models.append(model)\n",
    "    base_language_tokenizers.append(tokenizer)\n",
    "\n",
    "print(f\"\\nLoaded {len(base_language_models)} language models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d8ab4-8e9a-4a54-a847-ff947802562c",
   "metadata": {},
   "source": [
    "### MM-DES Inference Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2079c0bc-ea06-4f02-aa37-9272ef1a7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_dsel_outputs_text_tabular(\n",
    "    language_models,\n",
    "    tabular_models,\n",
    "    dsel_loader,\n",
    "    scaler_tabular,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute predictions on DSEL dataset for competence estimation\n",
    "    \"\"\"\n",
    "    dsel_preds = {\"language\": [], \"tabular\": []}\n",
    "    dsel_labels = []\n",
    "\n",
    "    for batch in dsel_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        tab = batch[\"tab\"].cpu().numpy()\n",
    "        labels = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "        dsel_labels.extend(labels)\n",
    "\n",
    "        # -----------------\n",
    "        # Language models\n",
    "        # -----------------\n",
    "        lang_batch = []\n",
    "        for model in language_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                probs = F.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                lang_batch.append(probs)\n",
    "\n",
    "        dsel_preds[\"language\"].append(\n",
    "            np.stack(lang_batch, axis=0)\n",
    "        )  # [n_models, batch, n_classes]\n",
    "\n",
    "        # -----------------\n",
    "        # Tabular models\n",
    "        # -----------------\n",
    "        tab_scaled = scaler_tabular.transform(tab)\n",
    "        tab_batch = []\n",
    "\n",
    "        for model in tabular_models:\n",
    "            probs = model.predict_proba(tab_scaled)\n",
    "            tab_batch.append(probs)\n",
    "\n",
    "        dsel_preds[\"tabular\"].append(\n",
    "            np.stack(tab_batch, axis=0)\n",
    "        )\n",
    "\n",
    "    # Concatenate batches\n",
    "    dsel_preds[\"language\"] = np.concatenate(dsel_preds[\"language\"], axis=1)\n",
    "    dsel_preds[\"tabular\"] = np.concatenate(dsel_preds[\"tabular\"], axis=1)\n",
    "\n",
    "    return dsel_preds, np.array(dsel_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f076065-4448-41b9-b5a0-4c0c04d76b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_des_weighted_text_tabular(\n",
    "    test_sample,\n",
    "    language_models,\n",
    "    tabular_models,\n",
    "    dsel_preds,\n",
    "    dsel_labels,\n",
    "    topk_indices,\n",
    "    scaler_tabular,\n",
    "    device=\"cuda\",\n",
    "    weights={\"language\": 0.5, \"tabular\": 0.5}\n",
    "):\n",
    "    \"\"\"\n",
    "    DES prediction using text + tabular models\n",
    "    \"\"\"\n",
    "\n",
    "    roc_labels = dsel_labels[topk_indices]\n",
    "\n",
    "    # -----------------\n",
    "    # Language competence\n",
    "    # -----------------\n",
    "    lang_comp = []\n",
    "    for i in range(len(language_models)):\n",
    "        preds = dsel_preds[\"language\"][i][topk_indices].argmax(1)\n",
    "        comp = (preds == roc_labels).mean()\n",
    "        lang_comp.append(comp)\n",
    "\n",
    "    # -----------------\n",
    "    # Tabular competence\n",
    "    # -----------------\n",
    "    tab_comp = []\n",
    "    for i in range(len(tabular_models)):\n",
    "        preds = dsel_preds[\"tabular\"][i][topk_indices].argmax(1)\n",
    "        comp = (preds == roc_labels).mean()\n",
    "        tab_comp.append(comp)\n",
    "\n",
    "    # Normalize competences\n",
    "    lang_w = torch.tensor(lang_comp, device=device)\n",
    "    tab_w = torch.tensor(tab_comp, device=device)\n",
    "\n",
    "    if lang_w.sum() > 0:\n",
    "        lang_w = lang_w / lang_w.sum()\n",
    "    if tab_w.sum() > 0:\n",
    "        tab_w = tab_w / tab_w.sum()\n",
    "\n",
    "    lang_w *= weights[\"language\"]\n",
    "    tab_w *= weights[\"tabular\"]\n",
    "\n",
    "    # -----------------\n",
    "    # Final fusion\n",
    "    # -----------------\n",
    "    combined = 0\n",
    "\n",
    "    # Language inference\n",
    "    input_ids = test_sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = test_sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "    for w, model in zip(lang_w, language_models):\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).logits\n",
    "        combined += w * logits\n",
    "\n",
    "    # Tabular inference\n",
    "    tab = test_sample[\"tab\"].cpu().numpy().reshape(1, -1)\n",
    "    tab_scaled = scaler_tabular.transform(tab)\n",
    "\n",
    "    for w, model in zip(tab_w, tabular_models):\n",
    "        probs = torch.tensor(\n",
    "            model.predict_proba(tab_scaled),\n",
    "            device=device\n",
    "        )\n",
    "        combined += w * probs\n",
    "\n",
    "    final_probs = combined / combined.sum(dim=1, keepdim=True)\n",
    "    pred = final_probs.argmax(dim=1).item()\n",
    "\n",
    "    return pred, final_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7b8b3-c95b-4d75-9e17-3c203b0e481f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "596a60ac-1d0a-4304-86a4-0e86a1fa5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Compute DSEL predictions once\n",
    "dsel_preds, dsel_labels = compute_dsel_outputs_text_tabular(\n",
    "    language_models=base_language_models,\n",
    "    tabular_models=base_tabular_models,\n",
    "    dsel_loader=val_loader,\n",
    "    scaler_tabular=scaler,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8befdc2f-bc6e-4239-8c0c-38127a50cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 1224/1224 [00:33<00:00, 36.39it/s]\n"
     ]
    }
   ],
   "source": [
    "results, true_labels, results_probs = [], [], []\n",
    "\n",
    "for test_sample in tqdm(test_ds):\n",
    "    topk_indices, _ = get_roc(\n",
    "        embedding_model,\n",
    "        test_sample,\n",
    "        dsel_embeddings,\n",
    "        logit_scale_exp,\n",
    "        k=10\n",
    "    )\n",
    "\n",
    "    y_pred, y_proba = predict_des_weighted_text_tabular(\n",
    "        test_sample,\n",
    "        base_language_models,\n",
    "        base_tabular_models,\n",
    "        dsel_preds,\n",
    "        dsel_labels,\n",
    "        topk_indices,\n",
    "        scaler,\n",
    "        device=device,\n",
    "        weights={\"language\": 0.4, \"tabular\": 0.6}\n",
    "    )\n",
    "\n",
    "    results.append(y_pred)\n",
    "    results_probs.append(y_proba.cpu().numpy())\n",
    "\n",
    "    label = test_sample[\"label\"]\n",
    "    true_labels.append(label.item() if torch.is_tensor(label) else label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a4d1a-7ed9-4c53-bf28-a4d7a76e05b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d74ed3f0-46dc-4a4c-8b03-2c6fed3b5622",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbd4858b-f5d3-4c90-a8b0-3d342a3969c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           0.7598\n",
      "F1 (macro):         0.7578\n",
      "Balanced Accuracy:  0.7598\n",
      "AUROC:              0.8386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array(true_labels)\n",
    "y_pred = np.array(results)\n",
    "\n",
    "# FIX HERE\n",
    "y_proba = np.array(results_probs).squeeze(1)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "# AUROC\n",
    "if y_proba.shape[1] == 2:\n",
    "    auroc = roc_auc_score(y_true, y_proba[:, 1])\n",
    "else:\n",
    "    auroc = roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"Accuracy:           {acc:.4f}\")\n",
    "print(f\"F1 (macro):         {f1:.4f}\")\n",
    "print(f\"Balanced Accuracy:  {bal_acc:.4f}\")\n",
    "print(f\"AUROC:              {auroc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d0d2e-20fb-429e-9e3d-64a36c952854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645a571-8ca7-4c98-90c3-10e32716e72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf3c6d-b7ce-4a53-9065-89bd4b96d533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
